<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>线性代数系列</title>
      <link href="/xian-xing-dai-shu-xi-lie.html"/>
      <url>/xian-xing-dai-shu-xi-lie.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>多层感知机</title>
      <link href="/duo-ceng-gan-zhi-ji.html"/>
      <url>/duo-ceng-gan-zhi-ji.html</url>
      
        <content type="html"><![CDATA[<p>最简单的深度网络称为多层感知机。多层感知机由多层神经元组成，每一层与它的上一层相连，从中接收输入；同时每一层也与它的下一层相连，影响当前层的神经元。</p><h2 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h2><p>网络中加入一个或多个隐藏层来客服线性模型的限制，简单的方法是将许多全连接层堆叠在一起。每一层都输出到上面的层，直到生成最后的输出。我们可以把前L−1层看作表示，把最后一层看作线性预测器。称为多层感知机（MLP）</p><p>多层感知机参数可能会很多，开销巨大。</p><p><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-08-15-32-31-image.png" alt></p><p><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-08-15-34-10-image.png" alt></p><p>在仿射变换后对每个隐藏单元应用非线性激活函数,激活函数的输出（例如，σ(·)）被称为活性值（activations）</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>激活函数（activation function）通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算。大多数激活函数都是非线性的。<strong>不管是单层感知机还是多个感知器，只要不带激活函数，都只能解决线性可分的问题</strong></p><h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p>给定元素x，ReLU函数被定义为该元素与0的最大值</p><p>使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题。</p><p>注意，ReLU函数有许多变体，包括参数化ReLU（Parameterized ReLU，pReLU）函数 (He et al., 2015)。该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过</p><p><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-08-15-43-57-image.png" alt></p><h3 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h3><p><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-08-15-44-54-image.png" alt></p><p>sigmoid在隐藏层中已经较少使用，它在大部分时候被更简单、更容易训练的ReLU所取代</p><h3 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h3><h2 id="多层感知机实现"><a href="#多层感知机实现" class="headerlink" title="多层感知机实现"></a>多层感知机实现</h2><p><code>torch.randn()</code> 返回一个符合均值为0，方差为1的<a href="https://so.csdn.net/so/search?q=%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83&spm=1001.2101.3001.7020" target="_blank" rel="noopener">正态分布</a>（标准正态分布）中填充随机数的张量,可以用在神经网络参数的初始化</p><p>torch.dataloader()可以指定batch_size均匀随机的加载数据</p><h2 id="模型选择、欠拟合和过拟合"><a href="#模型选择、欠拟合和过拟合" class="headerlink" title="模型选择、欠拟合和过拟合"></a>模型选择、欠拟合和过拟合</h2><p>泛化误差（generalization error）是指，模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望</p><p><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-10-16-02-55-image.png" alt></p><p><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-10-16-03-24-image.png" alt></p><p>欠拟合是指模型无法继续减少训练误差。过拟合是指训练误差远小于验证误差</p><p>由于不能基于训练误差来估计泛化误差，因此简单地最小化训练误差并不一定意味着泛化误差的减小。机器学习模型需要注意防止过拟合，即防止泛化误差过大。</p><p>验证集可以用于模型选择，但不能过于随意地使用它</p><p>解决过拟合的方法：</p><ol><li><p>获得更多的训练数据</p></li><li><p>选择合适的特征</p></li><li><p>减少参数的大小——正则化</p></li></ol><h2 id="权重选择"><a href="#权重选择" class="headerlink" title="权重选择"></a>权重选择</h2><p>在训练参数化机器学习模型时，权重衰减（weight decay）是最广泛使用的[正则化](<a href="https://zhuanlan.zhihu.com/p/67931198" target="_blank" rel="noopener">机器学习必知必会：正则化 - 知乎 (zhihu.com)</a>)的技术之一，它通常也被称为L2正则化。这项技术通过函数与零的距离来衡量函数的复杂度，因为在所有函数f中，函数f = 0（所有输入都得到值0）在某种意义上是最简单的。</p><p>要保证权重向量比较小，最常用方法是将其范数作为惩罚项加到最小化损失的问题中。将原来的训练目标最小化训练标签上的预测损失，调整为最小化预测损失和惩罚项之和</p><p>深度学习框架为了便于我们使用权重衰减，将权重衰减集成到优化算法中，以便与任何损失函数结合使用。此外，这种集成还有计算上的好处，允许在不增加任何额外的计算开销的情况下向算法中添加权重衰减。由于更新的权重衰减部分仅依赖于每个参数的当前值，因此优化器必须至少接触每个参数一次，</p><p><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-10-17-02-47-image.png" alt></p><p>weight_decay指定weight decay超参数</p><h2 id="暂退法（dropout）-深度学习中Dropout原理解析-知乎-zhihu-com"><a href="#暂退法（dropout）-深度学习中Dropout原理解析-知乎-zhihu-com" class="headerlink" title="[暂退法（dropout）](深度学习中Dropout原理解析 - 知乎 (zhihu.com))"></a>[暂退法（dropout）](<a href="https://zhuanlan.zhihu.com/p/38200980" target="_blank" rel="noopener">深度学习中Dropout原理解析 - 知乎 (zhihu.com)</a>)</h2><p>泛化性和灵活性之间的这种基本权衡被描述为偏差-方差权衡（bias‐variance tradeoff）。线性模型有很高的偏差：它们只能表示一小类函数。然而，这些模型的方差很低：它们在不同的随机数据样本上可以得出相似的结果。</p><p>好的预测模型：</p><p>模型简单（较小维度，平滑（对输入的微小变化不敏感））</p><p>在训练过程中，他们建议在计算后续层之前向网络的每一层注入噪声。因为当训练一个有多层的深层网络时，注入噪声只会在输入‐输出映射上增强平滑性。这个想法被称为暂退法（dropout）</p><p>对于深度学习框架的高级API，我们只需在每个全连接层之后添加一个Dropout层，将暂退概率作为唯一的参数传递给它的构造函数。在训练时，Dropout层将根据指定的暂退概率随机丢弃上一层的输出（相当于下一层的输入）。在测试时，Dropout层仅传递数据。</p><p><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-10-17-42-49-image.png" alt></p><h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p>前向传播指的是按顺序（从输入层到输出层）计算和存储神经网络中每层的结果</p><p>反向传播指的是计算神经网络参数梯度的方法。简言之，该方法根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。</p><h2 id="数值稳定性和模型初始化"><a href="#数值稳定性和模型初始化" class="headerlink" title="数值稳定性和模型初始化"></a>数值稳定性和模型初始化</h2><h3 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h3><p><strong>梯度爆炸（gradient exploding</strong>问题：参数更新过大，破坏了模型的稳定收敛；</p><p><strong>梯度消失（gradient vanishing）</strong> 问题：参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。</p><h4 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h4>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>线性神经网络</title>
      <link href="/xian-xing-shen-jing-wang-luo.html"/>
      <url>/xian-xing-shen-jing-wang-luo.html</url>
      
        <content type="html"><![CDATA[<p>回归（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系</p><p>线性回归基于几个简单的假设：首先，假设自变量x和因变量y之间的关系是线性的，即y可以表示为x中元素的加权和，这里通常允许包含观测值的一些噪声；其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。</p><p><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-04-15-21-37-image.png" alt></p><p>给定训练数据特征X和对应的已知标签y，线性回归的目标是找到一组权重向量w和偏置b：当给定从X的同分布中取样的新样本特征时，这组权重向量和偏置能够使得新样本预测标签的误差尽可能小。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>损失函数（loss function）能够量化目标的实际值与预测值之间的差距。通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。回归问题中最常用的损失函数是平方误差函数</p><p><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-04-15-58-43-image.png" alt></p><p>计算所有训练样本上的总损失</p><p><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-04-15-59-36-image.png" alt></p><p>训练时，希望寻找一组参数$ (w^*,b)$使得所有样本上的总损失最小。线性回归的有解析解</p><p><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-04-16-12-54-image.png" alt></p><p>推导见<a href="https://blog.csdn.net/a755199443/article/details/104198033" target="_blank" rel="noopener">[回归] 线性回归之解析解的推导过程_线性回归方程解析解求导-CSDN博客</a></p><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><p>梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值）关于模型参数的导数（在这里也可以称为梯度）。[梯度下降的理解](<a href="https://zhuanlan.zhihu.com/p/261375491" target="_blank" rel="noopener">Gradient-Descent（全世界最通俗易懂的梯度下降法详解-优化函数大法） - 知乎 (zhihu.com)</a>)但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本，这种变体叫做<strong>小批量随机梯度下降</strong>（mini batch stochastic gradient descent）。</p><p>在每次迭代中，我们首先随机抽样一个小批量B，它是由固定数量的训练样本组成的。然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。最后，我们将梯度乘以一个预先确定的正数η，并从当前参数的值中减掉</p><p><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-04-16-18-37-image.png" alt></p><p>算法的步骤如下：（1）初始化模型参数的值，如随机初始化；（2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤</p><p>η表示学习率（learning rate）。批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的。这些可以调整但不在训练过程中更新的参数称为超参(hyperparameter）。调参（hyperparameter tuning）是选择超参数的过程。</p><p>[随机梯度下降的理解](<a href="https://www.zhihu.com/question/27012077/answer/122359602" target="_blank" rel="noopener">(51 封私信 / 81 条消息) 为什么随机梯度下降方法能够收敛？ - 知乎 (zhihu.com)</a>)</p><h2 id="矢量化加速"><a href="#矢量化加速" class="headerlink" title="矢量化加速"></a>矢量化加速</h2><p>使用torch库的矢量相加比直接使用python运算快的多。</p><h3 id="动手实现"><a href="#动手实现" class="headerlink" title="动手实现"></a>动手实现</h3><ol><li><p><code>torch.matual()</code>是<a href="[torch.matmul()用法介绍-CSDN博客](https://blog.csdn.net/qsmx666/article/details/105783610)">tensor的乘法</a>，输入可以是高维的。<br>当输入都是二维时，就是普通的<a href="https://so.csdn.net/so/search?q=%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95&spm=1001.2101.3001.7020" target="_blank" rel="noopener">矩阵乘法</a>，和<code>tensor.mm</code>函数用法相同</p></li><li><p>可以使用pytorch的库加速模型的搭建。</p></li><li><p>这一单层被称为全连接层（fully‐connected layer），因为它的每一个输入都<br>通过矩阵‐向量乘法得到它的每个输出。</p></li><li><p>net = nn.Sequential(nn.Linear(2, 1))，linear就是一个线性层</p></li></ol><h2 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h2><p>为了解决分类问题等，是全连接层</p><p><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-08-11-27-00-image.png" alt></p><p><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-08-11-27-09-image.png" alt></p><p>输出不能直接视为概率</p><p><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-08-11-28-48-image.png" alt></p><p><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-08-11-30-27-image.png" alt></p><p>称为交叉熵损失</p><p><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-08-11-33-02-image.png" alt></p><p><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-08-13-56-07-image.png" alt></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>预备知识</title>
      <link href="/yu-bei-zhi-shi-1.html"/>
      <url>/yu-bei-zhi-shi-1.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>预备知识</title>
      <link href="/yu-bei-zhi-shi.html"/>
      <url>/yu-bei-zhi-shi.html</url>
      
        <content type="html"><![CDATA[<h2 id="pytorch基本操作"><a href="#pytorch基本操作" class="headerlink" title="pytorch基本操作"></a>pytorch基本操作</h2><ol><li><p>tensor 就是数组，可以通过 <code>x.shape</code>获取维度，x.size()获取大小<code>x.numel()</code>获取元素个数，<code>x.reshape()</code>改变张量形状，不改变元素数量和元素值。我们可以通过-1来调用此自动计算出维度的功能。即我们可以<br>用<code>x.reshape(-1,4</code>)或<code>x.reshape(3,-1)</code>来取代<code>x.reshape(3,4)</code>。<code>torch.zeros()</code>或<code>torch.ones()</code>可以初始化全0/全1数组，<code>torch.randn()</code>随机初始化一个数组</p></li><li><p>常见的标准算术运算符$（+,-,<em>,/和*</em>）$都可以对张量按元素逐个运算，也可以<code>torch.exp(x)</code>求幂运算，当然前提是元素维度相同。元素维度不同时要通过广播解决</p></li><li><p>如果我们想为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。例如，[0:2, :]访问第1行和第2行，其中“:”代表沿轴1（列）的所有元素。</p></li><li><p>执行原地操作，使用切片例如Y[:]=<experission>，从而减少内存开销</experission></p></li><li><p>tensor也可以转成python内置数据类型</p></li></ol><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>一般使用pandas预处理数据</p><ol><li>读取数据可以用<code>pd.read_csv</code>等</li><li>使用data.iloc[]进行切片处理可以分别获取不同列</li><li>fillna()可以填充NAN值</li><li><code>X = torch.tensor(inputs.to_numpy(dtype=float)</code>可以转换成张量形式</li></ol><h2 id="线性代数知识"><a href="#线性代数知识" class="headerlink" title="线性代数知识"></a>线性代数知识</h2><ol><li>张量是描述具有任意数量轴的n维数组的通用方法。例如，向量是一阶张量，矩阵是二阶张量。张量用特殊字体的大写字母表示（例如，X、Y和Z），它们的索引机制（与矩阵类似。</li><li>给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量。例如，将两个相同形状的矩阵相加，会在这两个矩阵上执行元素加法。两个矩阵的按元素乘法称为$Hadamard$积.</li><li><code>X.sum()</code>指定<code>axis=0</code>沿0轴按行降维求和，<code>axis=1</code> 按1轴按列降维求和，沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。</li><li>一个与求和相关的量是平均值（mean或average）。我们通过将总和除以元素总数来计算平均值。在代码中，我们可以调用函数来计算任意形状张量的平均值。<code>A.mean(), A.sum() / A.numel()</code></li><li><code>torch.dot(x,y)</code>可以求点积</li><li><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-04-14-05-21-image.png" alt></li><li>当我们为矩阵A和向量x调用torch.mv(A, x)时，会执行矩阵‐向量积</li><li><code>torch.mm()</code>矩阵乘法</li><li><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-04-14-21-13-image.png" alt></li></ol><p><img src="C:%5CUsers%5Cwjh%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2023-10-04-14-22-25-image.png" alt></p><h2 id="自动微分"><a href="#自动微分" class="headerlink" title="自动微分"></a>自动微分</h2><h2 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h2><p>具有多个特征时如果不同特征的取值范围相差过大会导致梯度下降过慢，需要特征缩放。方法</p><ol><li><p>数据除以最大值归一化一下；</p></li><li><p>数据都减去平均值再除以最大值减最小值</p></li><li><p>Z-score归一化</p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>基础</title>
      <link href="/ji-chu.html"/>
      <url>/ji-chu.html</url>
      
        <content type="html"><![CDATA[<ol><li>基础概念</li></ol><p>机器学习要解决的场景只需要定义一个灵活的程序算法，其输出由许多<em>参数</em>（parameter）决定，然后使用数据集来确定当下的“最佳参数集”，这些参数通过某种性能度量方式来达到完成任务的最佳性能。</p><p>参数可以被看作旋钮，旋钮的转动可以调整程序的行为。 任一调整参数后的程序被称为<strong><em>模型</em></strong>（model）。 通过操作参数而生成的所有不同程序（输入-输出映射）的集合称为“模型族”。 使用数据集来选择参数的元程序被称为<strong><em>学习算法</em>（</strong>learning algorithm）。</p><p>学习的过程：</p><ol><li><p>从一个随机初始化参数的模型开始，这个模型基本没有“智能”；</p></li><li><p>获取一些数据样本（例如，音频片段以及对应的是或否标签）；</p></li><li><p>调整参数，使模型在这些样本中表现得更好；</p></li><li><p>重复第（2）步和第（3）步，直到模型在任务中的表现令人满意。</p></li></ol><p><img src="https://zh-v2.d2l.ai/_images/ml-loop.svg" alt="/images/mlloopsvg"></p><h2 id="机器学习必备组件"><a href="#机器学习必备组件" class="headerlink" title="机器学习必备组件"></a>机器学习必备组件</h2><p>一般的必备组件：</p><ol><li><p>可以用来学习的<em>数据</em>（data）；</p></li><li><p>如何转换数据的<em>模型</em>（model）；</p></li><li><p>一个<em>目标函数</em>（objective function），用来量化模型的有效性；</p></li><li><p>调整模型参数以优化目标函数的<em>算法</em>（algorithm）。</p></li></ol><p>当每个样本的特征类别数量都是相同的时候，其特征向量是固定长度的，这个长度被称为数据的<em>维数</em>（dimensionality）</p><p>在机器学习中，我们需要定义模型的优劣程度的度量，这个度量在大多数情况是“可优化”的，这被称之为<em>目标函数</em>（objective function）。 我们通常定义一个目标函数，并希望优化它到最低点。 因为越低越好，所以这些函数有时被称为<em>损失函数</em>（loss function，或cost function）。</p><p>度学习中，大多流行的优化算法通常基于一种基本方法–<em>梯度下降</em>（gradient descent）。 简而言之，在每个步骤中，梯度下降法都会检查每个参数，看看如果仅对该参数进行少量变动，训练集损失会朝哪个方向移动。</p><h2 id="各种机器学习问题"><a href="#各种机器学习问题" class="headerlink" title="各种机器学习问题"></a>各种机器学习问题</h2><p><strong>监督学习</strong></p><p>监督学习的学习过程一般可以分为三大步骤：</p><ol><li><p>从已知大量数据样本中随机选取一个子集，为每个样本获取真实标签。有时，这些样本已有标签（例如，患者是否在下一年内康复？）；有时，这些样本可能需要被人工标记（例如，图像分类）。这些输入和相应的标签一起构成了训练数据集；</p></li><li><p>选择有监督的学习算法，它将训练数据集作为输入，并输出一个“已完成学习的模型”；</p></li><li><p>将之前没有见过的样本特征放到这个“已完成学习的模型”中，使用模型的输出作为相应标签的预测。</p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/shu-zi-tu-xiang-chu-li-di-er-zhang/shu-zi-tu-xiang-chu-li-2.4.html"/>
      <url>/shu-zi-tu-xiang-chu-li-di-er-zhang/shu-zi-tu-xiang-chu-li-2.4.html</url>
      
        <content type="html"><![CDATA[<h1 id="数字图像处理2-4"><a href="#数字图像处理2-4" class="headerlink" title="数字图像处理2.4"></a>数字图像处理2.4</h1><h2 id="图像的运算"><a href="#图像的运算" class="headerlink" title="图像的运算"></a>图像的运算</h2><ol><li><p>单点运算 </p><p><img src="G:%5CmyBlog%5CHexo%5Csource_posts%5C%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%AC%AC%E4%BA%8C%E7%AB%A0%5C1678234036929.jpg.jpg" alt="1678234036929.jpg"></p><p>求反–底片；甄别：按某灰度区分成黑白；下阶段低于某个值归零，高于不变；求反，如y=1-x；对比度伸展/压缩变换：取决于斜率；Gamma校正：人眼对光相对强度差敏感，<a href="https://zhuanlan.zhihu.com/p/37800433" target="_blank" rel="noopener">调色名词浅析——Gamma（伽玛）校正 - 知乎 (zhihu.com)</a>。</p></li><li><p>图像之间对应点运算：加减乘除等：<a href="https://blog.csdn.net/Rabbitbubu/article/details/54291335" target="_blank" rel="noopener">(5条消息) 图像基本运算_Rabbitbubu的博客-CSDN博客</a></p></li><li><p>领域运算：</p></li><li><p><img src="G:%5CmyBlog%5CHexo%5Csource_posts%5C%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%AC%AC%E4%BA%8C%E7%AB%A0%5C1678235566538.jpg.jpg" alt="1678235566538.jpg"></p></li><li><p>坐标变换</p></li></ol><h2 id="图像增强"><a href="#图像增强" class="headerlink" title="图像增强"></a>图像增强</h2><p>简单变换</p><h1 id="直方图处理3-3"><a href="#直方图处理3-3" class="headerlink" title="直方图处理3.3"></a>直方图处理3.3</h1><p>看亮度。横坐标r,纵坐标取值</p><p>直方图增强（是直方图均匀分布）：均衡化，规定化（与目的的增强目标）</p><p>直方变换：</p><ol><li>单调</li><li>排列次序不变</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/shu-zi-tu-xiang-chu-li-di-er-zhang/shu-zi-tu-xiang-chu-li-di-er-zhang.html"/>
      <url>/shu-zi-tu-xiang-chu-li-di-er-zhang/shu-zi-tu-xiang-chu-li-di-er-zhang.html</url>
      
        <content type="html"><![CDATA[<h1 id="数字图像处理第二章"><a href="#数字图像处理第二章" class="headerlink" title="数字图像处理第二章"></a>数字图像处理第二章</h1><h2 id="1-视觉基础"><a href="#1-视觉基础" class="headerlink" title="1. 视觉基础"></a>1. 视觉基础</h2><p>人眼结构：视网膜上感光细胞（锥细胞细节颜色敏感、柱细胞）</p><p><img src="G:%5CmyBlog%5CHexo%5Csource_posts%5C%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%AC%AC%E4%BA%8C%E7%AB%A0%5C1677631095249.jpg.jpg" alt="1677631095249.jpg"></p><p>HVS特性：</p><ol><li><p>空间频域低通线性系统，对太高频率不敏感</p></li><li><p>亮度相应对数非线性</p></li><li><p>对亮度信号的空间分辨能力大于对色度信号的空间分辨能力</p></li><li><p>边缘信息很重要</p></li><li><p>视觉遮掩效应是一种局部效应，受背景照度、纹理复杂性和信号频率的影响。</p></li><li><p>人眼主要看物体与周边物体亮度差异</p></li><li><p>亮度适应</p></li></ol><p>亮度对比计算公式：<br>$$<br>C_r=abs(B-B_0)/B_0*100%<br>$$<br>对比灵敏度：<br>$$<br>delta = 0.1<br>$$<br>单色（没有颜色的光视觉）模型：低通滤波器（人眼的光学系统）接高通滤波器（Mash效应），线性模型。</p><p>空间量化：均匀量化</p><p>亮度量化：非均匀，</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/opengl/opengl-xuan-ran-yuv-shu-ju.html"/>
      <url>/opengl/opengl-xuan-ran-yuv-shu-ju.html</url>
      
        <content type="html"><![CDATA[<h2 id="opengl渲染YUV数据"><a href="#opengl渲染YUV数据" class="headerlink" title="opengl渲染YUV数据"></a>opengl渲染YUV数据</h2><h3 id="1-yuv格式"><a href="#1-yuv格式" class="headerlink" title="1. yuv格式"></a>1. yuv格式</h3><p>现有宽为w高为h的一个视频，现在定义了一个data[] bytes变量并存储了这个视频的一帧数据。</p><p>bytes数组中，先连续存储每个像素的灰度信息，y通道占w<em>h个字节；<br>然后顺序存储颜色数据，u和v分别占一个字节，每个颜色数据共占2个字节，所以uv通道占(w/2)(h/2)2=w</em>h/2个字。</p><h4 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h4><p><code>YUV</code> （<code>Y&#39;CbCr</code>）是一种像素格式，常见于视频编码与静态图像。与 <code>RGB</code> 格式（红-绿-蓝）相反，YUV 分别由一个称为 <code>Y</code>（相当于灰度）的“亮度”分量（Luminance or Luma）和两个称为 <code>U</code>（蓝色投影 <code>Cb</code>）和 <code>V</code>（红色投影 <code>Cr</code>）的“色度”分量（Chrominance or Chroma）表示，由此得名。</p><p>仅有 Y 分量而没有 UV 分量信息，一样可以显示完整的黑白（灰度）图像，解决了模拟信号电视黑白与彩色的兼容问题。</p><p>色度通道（UV）的采样率可以低于亮度通道（Y），而不会显着降低感知质量。一种称为 “A:B:C” 的表示法用于描述相对于 Y 采样， U 和 V 的频率：</p><ul><li>4:4:4 表示不降低色度（UV）通道的采样率。每个 Y 分量对应一组 UV 分量。</li><li>4:2:2 表示 2:1 水平下采样，没有垂直下采样。每两个 Y 分量共享一组 UV 分量。</li><li>4:2:0 表示 2:1 水平下采样，同时 2:1 垂直下采样。每四个 Y 分量共享一组 UV 分量。</li><li>4:1:1 表示 4:1 水平下采样，没有垂直下采样。每四个 Y 分量共享一组 UV 分量。4:1:1 采样比其他格式少见，本文不再详细讨论。</li></ul><p>下图显示了如何针对每个下采样率采样色度。亮度样本用十字表示，色度样本用圆圈表示。</p><p><img src="G:%5CmyBlog%5CHexo%5Csource_posts%5Copengl%5Cwebp" alt="img"></p><h4 id="存储格式"><a href="#存储格式" class="headerlink" title="存储格式"></a>存储格式</h4><p>YUV 在存储上通常分为平面格式（<code>Planar</code>），半平面格式（<code>Semi-Planar</code>）以及打包格式（<code>Packed</code>）。</p><p><code>YU12</code> 即 <code>I420</code>，也叫 <code>IYUV</code>，属于 <code>YUV420P</code> 格式。三个平面，分别存储 Y U V 分量。每四个 Y 分量共享一组 UV 分量。U、V 平面的 strides, width 和 height 都是 Y 平面的一半，因此一个像素 12 bits，内存排列如下图所示：</p><p><img src="G:%5CmyBlog%5CHexo%5Csource_posts%5Copengl%5Cyuv" alt="img"></p><h3 id="2-纹理"><a href="#2-纹理" class="headerlink" title="2. 纹理"></a>2. 纹理</h3><h4 id="2-1-纹理组成"><a href="#2-1-纹理组成" class="headerlink" title="2.1 纹理组成"></a>2.1 纹理组成</h4><ul><li>y通道的数据生成一个opengl纹理；</li><li>uv通道的数据生成一个opengl纹理；</li><li>两个纹理作为渲染时的输入，在渲染过程中由GPU组装还原出来视频画面。</li></ul><p>yv12是三个纹理是y一个，u一个，v一个；nv12是y一个，uv一个。</p><h4 id="2-2-纹理格式"><a href="#2-2-纹理格式" class="headerlink" title="2.2 纹理格式"></a>2.2 纹理格式</h4><ul><li>GL_LUMINANCE 纹理用来加载 NV21 Y Plane 的数据；</li><li>GL_LUMINANCE_ALPHA 纹理用来加载 UV Plane 的数据。</li></ul><h3 id="3-渲染步骤"><a href="#3-渲染步骤" class="headerlink" title="3. 渲染步骤"></a>3. 渲染步骤</h3><ul><li><p>第一步：初始化上下文</p></li><li><p>第二步：初始化着色器程序（编译链接着色器程序，生成2个纹理–&gt;确定纹理坐标及对应的顶点坐标–&gt;加载纹理坐标和顶点坐标数据到着色器程序）；</p></li><li><p>第三步：初始化buffer；</p></li><li><p>第四步：初始化纹理（分别加载NV21的两个Plane数据到2个纹理）；</p></li><li><p>第五步：绘制。</p><h3 id="4-代码实现"><a href="#4-代码实现" class="headerlink" title="4. 代码实现"></a>4. 代码实现</h3></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>力扣刷题之链表</title>
      <link href="/li-kou-shua-ti-zhi-lian-biao.html"/>
      <url>/li-kou-shua-ti-zhi-lian-biao.html</url>
      
        <content type="html"><![CDATA[<h1 id="1-链表基础"><a href="#1-链表基础" class="headerlink" title="1. 链表基础"></a>1. 链表基础</h1><p><img src="%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98%E4%B9%8B%E9%93%BE%E8%A1%A8/20200806194529815.png" alt="链表1"></p>]]></content>
      
      
      <categories>
          
          <category> 刷题记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>client源码分析</title>
      <link href="/client-yuan-ma-fen-xi.html"/>
      <url>/client-yuan-ma-fen-xi.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>WebRTC peerconnection分析</title>
      <link href="/webrtc-peerconnection-fen-xi.html"/>
      <url>/webrtc-peerconnection-fen-xi.html</url>
      
        <content type="html"><![CDATA[<h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h1><p>peerconnection example由peerconnection server和client两部分组成，重点分析client。</p><h1 id="2-类关系"><a href="#2-类关系" class="headerlink" title="2. 类关系"></a>2. 类关系</h1><p>peerconnection_client 是一个用于通话的客户端，核心类为 <strong>Conductor</strong>，它主要包含了 <strong>MainWindow</strong> 和 <strong>PeerConnectionClient</strong> 两个核心类对象（实际上它是一个中转调用的对象），前者是处理 Windows 窗口和消息的类，后者是与信令服务器交互的类，类关系图如下。</p><p><img src="WebRTC-peerconnection%E5%88%86%E6%9E%90/-16706675490483.png" alt="img"></p><p><strong>main_wnd.cc</strong>中实现了四个类，<code>MainWindow</code>是接口类，定义了用于Windows操作界面的切换和视频渲染的接口，<code>MianWin</code>继承了他并实现了接口的方法；<code>VideoRenderer</code>类用于渲染视频帧，并被<code>MainWin</code>拥有（作为MainWin类的成员）。</p><p><strong>conductor.cc</strong>实现了conductor类，实际上使用了观察者模式。</p><p><img src="WebRTC-peerconnection%E5%88%86%E6%9E%90/-16706675747795.png" alt="img"></p><p>xxx_Observer代表是一个观察者，当观察的目标发生变化时，<strong>就会通知xxx_Observer产生一个事件回调</strong>。Conductor继承了xxx_Observer，所以Conductor也是一个观察者身份。因此Conductor会存在很多On_XXXX相关的实际回调。同时由<strong>于conductor拥有MianWind和PeerConnection对象的指针，</strong>因此它main_wnd，Peer_connection_client 也实现了一个观察者模式。Conductor 监听到事件，会通知main_wnd Peer_connection_client进行相关界面更新或者与peer的进一步交互，在这个模式中，Conductor是目标，main_wnd和Peer_connection_client是观察者角色，同时这里还有一个妙用，观察者通过回调，也能调用目标者的相关API。</p><p><strong>peer_connection_client.cc</strong>实现了PeerConnectionClient类和PeerConnectionClientObserver类。用于信令处理与收发。</p>]]></content>
      
      
      <categories>
          
          <category> WebRTC </category>
          
      </categories>
      
      
        <tags>
            
            <tag> WebRTC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>个人博客搭建踩坑记录</title>
      <link href="/ge-ren-bo-ke-da-jian.html"/>
      <url>/ge-ren-bo-ke-da-jian.html</url>
      
        <content type="html"><![CDATA[<h2 id="1-node环境"><a href="#1-node环境" class="headerlink" title="1.node环境"></a>1.node环境</h2><p>需要node.js环境，需要注意的是高版本node和hexo版本不兼容，导致hexo运行时报奇怪的错误，因此建议使用nvm进行包管理。可以参考：</p><blockquote><p><a href="https://blog.csdn.net/Y1914960928/article/details/121748603" target="_blank" rel="noopener">https://blog.csdn.net/Y1914960928/article/details/121748603</a></p></blockquote><p>经测试node12可用。</p><p>安装完node后执行以下命令安装hexo</p><pre><code>npm install hexo-cli -g</code></pre><h2 id="2-本地博客初始化"><a href="#2-本地博客初始化" class="headerlink" title="2.本地博客初始化"></a>2.本地博客初始化</h2><p>新建一个my_blog目录，在命令行中输入</p><pre><code>hexo initnpm i</code></pre><p>无错误信息表示成功，在命令行中</p><pre><code>hexo g &amp;&amp; hexo s</code></pre><p>即可生成静态网页并在本地预览效果。</p><h2 id="3-个性化改造"><a href="#3-个性化改造" class="headerlink" title="3.个性化改造"></a>3.个性化改造</h2><p>hexo的核心配置在 _config.yml下，可以在hexo官网中选择各种好看的主题clone至theme目录下，修改博客根目录下的config文件即可。具体主题的配置可以看主题作者的教程，核心依然是主题下的_config.yml，可以尽情探索。</p><h2 id="4-部署至gitHub-io"><a href="#4-部署至gitHub-io" class="headerlink" title="4.部署至gitHub.io"></a>4.部署至gitHub.io</h2><p>这里注意xxx.github.io这个仓库不能是空的即可。</p>]]></content>
      
      
      <categories>
          
          <category> 杂谈 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 个人博客 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/hello-world.html"/>
      <url>/hello-world.html</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习基础概念</title>
      <link href="/1.ji-chu-gai-nian.html"/>
      <url>/1.ji-chu-gai-nian.html</url>
      
        <content type="html"><![CDATA[<h2 id="1-基础概念"><a href="#1-基础概念" class="headerlink" title="1. 基础概念"></a>1. 基础概念</h2><p>机器学习要解决的场景只需要定义一个灵活的程序算法，其输出由许多<em>参数</em>（parameter）决定，然后使用数据集来确定当下的“最佳参数集”，这些参数通过某种性能度量方式来达到完成任务的最佳性能。</p><p>参数可以被看作旋钮，旋钮的转动可以调整程序的行为。 任一调整参数后的程序被称为<strong><em>模型</em></strong>（model）。 通过操作参数而生成的所有不同程序（输入-输出映射）的集合称为“模型族”。 使用数据集来选择参数的元程序被称为<strong><em>学习算法</em>（</strong>learning algorithm）。</p><p>学习的过程：</p><ol><li><p>从一个随机初始化参数的模型开始，这个模型基本没有“智能”；</p></li><li><p>获取一些数据样本（例如，音频片段以及对应的是或否标签）；</p></li><li><p>调整参数，使模型在这些样本中表现得更好；</p></li><li><p>重复第（2）步和第（3）步，直到模型在任务中的表现令人满意。</p></li></ol><p><img src="https://zh-v2.d2l.ai/_images/ml-loop.svg" alt="../_images/ml-loop.svg"></p><h2 id="机器学习必备组件"><a href="#机器学习必备组件" class="headerlink" title="机器学习必备组件"></a>机器学习必备组件</h2><p>一般的必备组件：</p><ol><li><p>可以用来学习的<em>数据</em>（data）；</p></li><li><p>如何转换数据的<em>模型</em>（model）；</p></li><li><p>一个<em>目标函数</em>（objective function），用来量化模型的有效性；</p></li><li><p>调整模型参数以优化目标函数的<em>算法</em>（algorithm）。</p></li></ol><p>当每个样本的特征类别数量都是相同的时候，其特征向量是固定长度的，这个长度被称为数据的<em>维数</em>（dimensionality）</p><p>在机器学习中，我们需要定义模型的优劣程度的度量，这个度量在大多数情况是“可优化”的，这被称之为<em>目标函数</em>（objective function）。 我们通常定义一个目标函数，并希望优化它到最低点。 因为越低越好，所以这些函数有时被称为<em>损失函数</em>（loss function，或cost function）。</p><p>度学习中，大多流行的优化算法通常基于一种基本方法–<em>梯度下降</em>（gradient descent）。 简而言之，在每个步骤中，梯度下降法都会检查每个参数，看看如果仅对该参数进行少量变动，训练集损失会朝哪个方向移动。</p><h2 id="各种机器学习问题"><a href="#各种机器学习问题" class="headerlink" title="各种机器学习问题"></a>各种机器学习问题</h2><p><strong>监督学习</strong></p><p>监督学习的学习过程一般可以分为三大步骤：</p><ol><li><p>从已知大量数据样本中随机选取一个子集，为每个样本获取真实标签。有时，这些样本已有标签（例如，患者是否在下一年内康复？）；有时，这些样本可能需要被人工标记（例如，图像分类）。这些输入和相应的标签一起构成了训练数据集；</p></li><li><p>选择有监督的学习算法，它将训练数据集作为输入，并输出一个“已完成学习的模型”；</p></li><li><p>将之前没有见过的样本特征放到这个“已完成学习的模型”中，使用模型的输出作为相应标签的预测。</p></li><li></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
